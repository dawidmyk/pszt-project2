<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">
<!-- saved from url=(0070)https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-2">
<title>Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a</title>
<meta name="description" content="Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a">
<meta name="keywords" content="c4_5Main">
<meta name="resource-type" content="document">
<meta name="distribution" content="global">


<meta name="Generator" content="jLaTeX2HTML v2002 JA patch-1.4">
<meta http-equiv="Content-Style-Type" content="text/css">

<link rel="STYLESHEET" href="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/c4_5Main.css">

</head>

<body>
<!--Navigation Panel-->
<img width="81" height="24" align="BOTTOM" border="0" alt="next_inactive" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/nx_grp_g.png"> 
<img width="26" height="24" align="BOTTOM" border="0" alt="up" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/up_g.png"> 
<img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/prev_g.png">   
<br>
<br>
<br>
<!--End of Navigation Panel-->

  <h1 align="CENTER">Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a</h1>
<p align="CENTER"><strong>Urszula Herman-I¿ycka 
<br>
Przemys³aw Strzelczak</strong></p>
<p align="CENTER"><strong>19. marca 2004</strong></p>
  <br>

<h2><a name="SECTION00010000000000000000">
Spis rzeczy</a>
</h2>
<!--Table of Contents-->

<ul>
<li><a name="tex2html32" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html">Wprowadzenie</a>
</li><li><a name="tex2html33" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00030000000000000000">Definicja drzewa decyzyjnego</a>
</li><li><a name="tex2html34" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00040000000000000000">Pomocnicze definicje</a>
<ul>
<li><a name="tex2html35" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00041000000000000000">Entropia rozk³adu prawdopodobieñstwa</a>
</li><li><a name="tex2html36" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00042000000000000000">Podzia³ na klasy</a>
</li><li><a name="tex2html37" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00043000000000000000">Informacja atrybutu</a>
</li><li><a name="tex2html38" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00044000000000000000">Gain(X,T)</a>
</li><li><a name="tex2html39" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00045000000000000000">Gain Ratios</a>
</li></ul>
<br>
</li><li><a name="tex2html40" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00050000000000000000">Algorytm ID3</a>
</li><li><a name="tex2html41" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00060000000000000000">Cechy algorytmu ID3</a>
</li><li><a name="tex2html42" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00070000000000000000">C4.5, czyli dlaczego ID3 nam nie wystarczy?</a>
<ul>
<li><a name="tex2html43" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00071000000000000000">Lekarstwo na techniczne ograniczenia ID3</a>
</li><li><a name="tex2html44" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00072000000000000000">Przycinanie, czyli g³ówny pomys³ C4.5</a>
</li></ul>
<br>
</li><li><a name="tex2html45" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00080000000000000000">Implementacja algorytmu C4.5</a>
<ul>
<li><a name="tex2html46" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00081000000000000000">Nazewnictwo</a>
</li><li><a name="tex2html47" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00082000000000000000">Wyniki</a>
</li></ul>
<br>
</li><li><a name="tex2html48" href="https://www.mimuw.edu.pl/~awojna/SID/referaty/strzelczak/c4_5Main.html#SECTION00090000000000000000">About this document ...</a>
</li></ul>
<!--End of Table of Contents-->  
<h1><a name="SECTION00020000000000000000">
Wprowadzenie</a>
</h1>

<p>

</p><h4><a name="SECTION00020010000000000000">
Idea</a>
</h4>
Cz³owiek posiada tê umiejêtno¶æ, ¿e na podstawie przesz³ych do¶wiadczeñ
potrafi trafnie sklasyfikowaæ nowy przypadek i podj±æ wobec niego 
dobr± decyzjê. My wiêc chcieliby¶my tego samego nauczyæ komputer.
Daj±c mu ''baga¿'' do¶wiadczeñ, czyli informacjê o przypadkach 
okre¶lonego typu, chcieliby¶my, aby podejmowa³ racjonaln± decyzjê.
Racjonaln± znaczy najbli¿sz± indukowanym regu³om, które w jego mniemaniu 
zosta³y u¿yte do podjêcia tych dobrych decyzji i jednocze¶nie o
niskim przewidywalnym poziomie pomy³ek.

<p>
Do tych w³a¶nie celów wymy¶lono <i>drzewa decyzyjne</i>, które na sta³e 
wpisa³y siê w poczet elementów uczenia maszynowego.
Na podstawie dostarczonego zbioru faktów i regu³ maszyna uczy siê jak 
sklasyfikowaæ nowe przypadki. Zbiór faktów na podstawie, których bêdziemy
wnioskowaæ nazywamy <tt>Training Set</tt>, natomiast nowe przypadki, które
bêdziemy chcieli zaklasyfikowaæ to <tt>Test Set</tt>.
Klasyfikacja polega na stwierdzeniu w jakiej kategorii umie¶ciæ nowy przypadek,
zwykle jest to podzia³ binarny na <tt>true</tt> lub <tt>false</tt> itp.
Training Set jest zbiorem rekordów o tej samej strukturze, na któr± sk³adaj± 
siê pary typu atrybut/warto¶æ atrybutu. Ponadto ka¿dy rekord jest 
przyporz±dkowany do odpowiedniej kategorii. Na podstawie warto¶ci tych atrybutów 
i Training Set próbujemy sklasyfikowaæ nowe przypadki, w których
mamy dane jedynie atrybuty i ich warto¶ci.

</p><p>
Oto przyk³ad wziêty z gie³dy:

</p><p>
</p><pre>	ATTRIBUTE   |	POSSIBLE VALUES
	============+=======================
	age	    | old, midlife, new
	------------+-----------------------
	competition | no, yes
	------------+-----------------------
	type        | software, hardware
	------------+-----------------------


	AGE	| COMPETITION | TYPE	| PROFIT
	=========================================
	old	| yes	      | swr	| down
	--------+-------------+---------+--------
	old	| no	      | swr 	| down
	--------+-------------+---------+--------
	old	| no	      | hwr	| down
	--------+-------------+---------+--------
	mid	| yes	      | swr	| down
	--------+-------------+---------+--------
	mid	| yes	      | hwr	| down
	--------+-------------+---------+--------
	mid	| no	      | hwr	| up
	--------+-------------+---------+--------
	mid	| no	      | swr	| up
	--------+-------------+---------+--------
	new	| yes	      | swr	| up
	--------+-------------+---------+--------
	new	| no	      | hwr	| up
	--------+-------------+---------+--------
	new	| no	      | swr	| up
	--------+-------------+---------+--------
</pre>

<p>
A tak¿e przyk³ad golfowy, z którego bêdziemy czêsto korzystaæ. Na podstawie
warunków atmosferycznych okre¶lamy czy bêdziemy graæ w golfa czy nie.

</p><p>
</p><pre>	ATTRIBUTE   |	POSSIBLE VALUES
	============+=======================
	outlook	    | sunny, overcast, rain
	------------+-----------------------
	temperature | continuous
	------------+-----------------------
	humidity    | continuous
	------------+-----------------------
	windy       | true, false
	============+=======================


	OUTLOOK | TEMPERATURE | HUMIDITY | WINDY | PLAY
	=====================================================
	sunny   |      85     |    85    | false | Don't Play
	sunny   |      80     |    90    | true  | Don't Play
	overcast|      83     |    78    | false | Play
	rain    |      70     |    96    | false | Play
	rain    |      68     |    80    | false | Play
	rain    |      65     |    70    | true  | Don't Play
	overcast|      64     |    65    | true  | Play
	sunny   |      72     |    95    | false | Don't Play
	sunny   |      69     |    70    | false | Play
	rain    |      75     |    80    | false | Play
	sunny   |      75     |    70    | true  | Play
	overcast|      72     |    90    | true  | Play
	overcast|      81     |    75    | false | Play
	rain    |      71     |    80    | true  | Don't Play
</pre>
  
<h1><a name="SECTION00030000000000000000">
Definicja drzewa decyzyjnego</a>
</h1>

<p>
Drzewo decyzyjne jest odpowiednio zbudowanym drzewem o nastêpuj±cych 
w³asno¶ciach:

</p><ul>
<li>W ka¿dym wê¼le drzewa umieszczony jest jeden z atrybutów.
</li>
<li>Ka¿da krawêd¼ wychodz±ca z danego wêz³a jest etykietowana jedn± z mo¿liwych
warto¶ci atrybutu ojca.
</li>
<li>Li¶ciem w takim drzewie jest warto¶æ ze zbioru kategorii, jak± 
przyporz±dkujemy rekordom maj±cym takie warto¶ci, jakie znajduj± siê na ¶cie¿ce
od korzenia do li¶cia.
</li>
<li>Na ka¿dym poziomie w drzewie mog± siê znajdowaæ zarówno wêz³y z atrybutami, 
jak i li¶cie.
</li>
</ul>

<p>
Ponadto dobrze zbudowane drzewo decyzyjne 
(tzn. takie, które szybko potrafi zwróciæ kategoriê) 
ma odpowiednio rozmieszczone atrybuty w wêz³ach:

</p><ul>
<li>W korzeniu znajduje siê atrybut, który zawiera ,,najwiêcej informacji''
(tzn. jego warto¶ci maj± wiêkszy wp³yw na wynik ni¿ pozosta³e)
</li>
<li>Dalej drzewo budowane jest rekurencyjnie zgodnie z zasad± umieszczania
w korzeniu atrybutów z ,,najwiêksz± informacj±''.
</li>
</ul>

<p>
Drzewo decyzyjne w przyk³adzie gie³dowym:
</p><pre>			 Age
		       / |    \
		      /  |     \
		  new/   |mid   \old
		    /    |       \
		  Up  Competition Down
                       /      \
		      /        \
		   no/          \yes
		    /            \
		  Up             Down
</pre>

<p>
Natomiast w przyk³adzie golfowym drzewo decyzyjne wygl±da tak:
(wierz±c autorowi artyku³u :-)
</p><pre>			Outlook
		       / |     \
		      /  |      \
            overcast /   |sunny  \rain
                    /    |        \
	         Play   Humidity   Windy
		       /   |         |  \
                      /    |         |   \
		&lt;=75 /  &gt;75|     true|    \false
		    /      |         |     \
                 Play   Don'tPlay Don'tPlay Play
</pre>
  
<h1><a name="SECTION00040000000000000000">
Pomocnicze definicje</a>
</h1>

<p>
Aby przyst±piæ do budowy drzewa potrzebujemy jeszcze kilku definicji, a przede
wszystkim okre¶liæ jak wybraæ spo¶ród atrybutów ten, który zawiera ,,najwiêcej
informacji''.

</p><p>

</p><h2><a name="SECTION00041000000000000000">
Entropia rozk³adu prawdopodobieñstwa</a>
</h2>

<p>
Maj±c zadany rozk³ad prawdopodobieñstwa P=(p1, p2, ..., pn) okre¶lamy 
<tt>Informacjê, jak± przechowuje ten rozk³ad</tt> (<tt>Entropiê P</tt>) jako:

</p><p>
<!-- MATH
 $I(P) = -(p_1* \log(p_1) + p_2* \log(p_2) + \ldots + p_n* \log(p_n))$
 -->
<img width="464" height="37" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img1.png" alt="$I(P) = -(p_1* \log(p_1) + p_2* \log(p_2) + \ldots + p_n* \log(p_n))$">

</p><p>
Im bardziej jednorodny rozk³ad prawdopodobieñstwa, tym wiêksza jest jego
informacja:

</p><ul>
<li>P=(0.5, 0.5)   I(P)=1
</li>
<li>P=(0.67, 0.33) I(P)=0.92
</li>
<li>P=(1, 0)       I(P)=0
</li>
</ul>

<p>

</p><h2><a name="SECTION00042000000000000000">
Podzia³ na klasy</a>
</h2>

<p>
Podzielmy zbiór rekordów T na roz³±czne klasy C1, C2, ..., Ck wed³ug 
warto¶ci ich atrybutu kategorii. Informacjê potrzebn±, by stwierdziæ do której
klasy nale¿y element ze zbioru oznaczmy przez 

</p><p>
<!-- MATH
 $Info(T) = I(P)$
 -->
<img width="138" height="37" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img2.png" alt="$Info(T) = I(P)$">

</p><p>
gdzie P jest rozk³adem prawdopodobieñstwa podzia³u (C1, C2, ..., Ck) zadanym
nastêpuj±co

</p><p>
<!-- MATH
 $P = (\frac{C_1}{T}, \frac{C_2}{T}, \ldots, \frac{C_k}{T})$
 -->
<img width="170" height="41" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img3.png" alt="$P = (\frac{C_1}{T}, \frac{C_2}{T}, \ldots, \frac{C_k}{T})$">

</p><p>
W przyk³adzie z gie³d± mamy:

</p><p>
<!-- MATH
 $Info(T) = I(\frac{5}{10},\frac{5}{10}) = 1.0$
 -->
<img width="215" height="39" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img4.png" alt="$Info(T) = I(\frac{5}{10},\frac{5}{10}) = 1.0$">

</p><p>
gdy¿ mamy dwie klasy (Up, Down) i w naszym Training Set dok³adnie 5 entek
spo¶ród 10 mia³o kategoriê Up i dok³adnie 5- Down.

</p><p>

</p><h2><a name="SECTION00043000000000000000">
Informacja atrybutu</a>
</h2>

<p>
Podzielmy zbiór T na podstawie warto¶ci atrybutu X na podzbiory T1, T2, .., Tn.
Informacja potrzebna, by stwierdziæ do jakiej klasy nale¿y element z T jest
¶redni± wa¿on± informacji potrzebnych, by stwierdziæ do jakiej klasy nale¿y
element Ti:

</p><p>
<!-- MATH
 $Info(X,T) = \sum_{i = 1}^{n} \frac{\|T_i\|}{\|T\|} * Info(T_i)$
 -->
<img width="287" height="45" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img5.png" alt="$Info(X,T) = \sum_{i = 1}^{n} \frac{\Vert T_i\Vert}{\Vert T\Vert} * Info(T_i)$">

</p><p>
W golfowym przyk³adzie obliczenia s± nastêpuj±ce:

</p><p>
<!-- MATH
 $Info(Outlook,T) = \frac{5}{14} * I(\frac{2}{5}, \frac{3}{5}) + 
                   \frac{4}{14} * I(\frac{4}{4},0) + 
                   \frac{5}{14} * I(\frac{3}{5}, \frac{2}{5}) = 0.694$
 -->
<img width="554" height="39" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img6.png" alt="$Info(Outlook,T) = \frac{5}{14} * I(\frac{2}{5}, \frac{3}{5}) +
\frac{4}{14} * I(\frac{4}{4},0) +
\frac{5}{14} * I(\frac{3}{5}, \frac{2}{5}) = 0.694$">

</p><p>

</p><ul>
<li>5 * <tt>sunny</tt> =&gt; 2 * <tt>play</tt>
</li>
<li>4 * <tt>overcast</tt> =&gt; 4 * <tt>play</tt>
</li>
<li>5 * <tt>rain</tt> =&gt; 3 * <tt>play</tt>
</li>
</ul>

<p>

</p><h2><a name="SECTION00044000000000000000">
Gain(X,T)</a>
</h2>

<p>
Ró¿nicê pomiêdzy informacj± potrzebn±, by stwierdziæ do jakiej klasy nale¿y 
element T, a informacj± potrzebn±, po tym jak poznali¶my warto¶æ atrybutu X
definiujemy jako:

</p><p>
<!-- MATH
 $Gain(X,T) = Info(T) - Info(X,T)$
 -->
<img width="311" height="37" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img7.png" alt="$Gain(X,T) = Info(T) - Info(X,T)$">

</p><p>
W przyk³adzie golfowym:

</p><p>
<!-- MATH
 $Gain(Outlook,T) = Info(T) - Info(Outlook,T) = 0.94 - 0.694 = 0.246$
 -->
<img width="602" height="37" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img8.png" alt="$Gain(Outlook,T) = Info(T) - Info(Outlook,T) = 0.94 - 0.694 = 0.246$">

</p><p>
<!-- MATH
 $Gain(Windy,T) = Info(T) - Info(Windy,T) = 0.94 - 0.892 = 0.048$
 -->
<img width="584" height="37" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img9.png" alt="$Gain(Windy,T) = Info(T) - Info(Windy,T) = 0.94 - 0.892 = 0.048$">

</p><p>
Widzimy wiêc, ¿e Outlook daje wiêkszy zysk informacji ni¿ Windy.

<br>
Buduj±c drzewo decyzyjne bêdziemy siê opieraæ na warto¶ciach <tt>Gain</tt> 
wyliczonych dla ka¿dego atrybutu i umieszczaæ w korzeniu poddrzewa ten
atrybut spo¶ród tych, które jeszcze nie zosta³y umieszczone w drzewie a który
ma najwiêksz± warto¶æ <tt>Gain</tt>. Dziêki temu zbudujemy ma³e drzewo decyzyjne,
które bêdzie w stanie sklasyfikowaæ odpowiednio nowe rekordy w kilku krokach.

</p><p>

</p><h2><a name="SECTION00045000000000000000">
Gain Ratios</a>
</h2>

<p>
Gain daje lepsze wyniki w przypadku, gdy atrybut ma du¿± liczbê ro¿nych 
warto¶ci. Na przyk³ad dla atrybutu D, który ma rozk³ad prawdopodobieñstwa 
(0,1) Info(D,T)=0 za¶ Gain(D,T) jest maksymalne i równe Info(T). Problem mo¿emy 
zaobserwowaæ w naszym przyk³adzie golfowym, Gain(Outlook,T) ma du¿o wiêksz± warto¶æ
ni¿ Gain(Windy,T), gdy¿ Outlook przyjmuje trzy warto¶ci a Windy tylko dwie.
Aby to zrównowa¿yæ wprowadzono <tt>Gain Ratio</tt>:

</p><p>
<!-- MATH
 $GainRatio(D,T) = \frac{Gain(D,T)}{SplitInfo(D,T)}$
 -->
<img width="269" height="45" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img10.png" alt="$GainRatio(D,T) = \frac{Gain(D,T)}{SplitInfo(D,T)}$">

</p><p>
gdzie SplitInfo(D,T) definiujemy nastêpuj±co (zak³adaj±c ¿e T1..Tm jest 
podzia³em T indukowanym przez D):

</p><p>
<!-- MATH
 $SplitInfo(D,T) = I(\frac{T1}{T}, \frac{T2}{T}, \ldots, \frac{Tm}{T})$
 -->
<img width="305" height="40" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img11.png" alt="$SplitInfo(D,T) = I(\frac{T1}{T}, \frac{T2}{T}, \ldots, \frac{Tm}{T})$">

</p><p>
W przyk³adzie golfowym mamy:

</p><ul>
<li><!-- MATH
 $SplitInfo(Outlook,T) = -\frac{5}{14} * \log(\frac{5}{14}) - 
                               \frac{4}{14} * \log(\frac{4}{14}) - 
                               \frac{5}{14} * \log(\frac{5}{14}) = 1.577$
 -->
<img width="617" height="40" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img12.png" alt="$SplitInfo(Outlook,T) = -\frac{5}{14} * \log(\frac{5}{14}) -
\frac{4}{14} * \log(\frac{4}{14}) -
\frac{5}{14} * \log(\frac{5}{14}) = 1.577$"> 

<br><!-- MATH
 $GainRatio(Outlook,T) = \frac{0.246}{1.577} = 0.156$
 -->
<img width="324" height="39" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img13.png" alt="$GainRatio(Outlook,T) = \frac{0.246}{1.577} = 0.156$">
</li>
<li><!-- MATH
 $SplitInfo(Windy,T) = -\frac{6}{14} * \log(\frac{6}{14}) - 
                             \frac{8}{14} * \log(\frac{8}{14}) = 
                             \frac{6}{14} * 0.1.222 + \frac{8}{14} * 0.807 = 0.985$
 -->
<img width="712" height="40" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img14.png" alt="$SplitInfo(Windy,T) = -\frac{6}{14} * \log(\frac{6}{14}) -
\frac{8}{14} * \log(\frac{8}{14}) =
\frac{6}{14} * 0.1.222 + \frac{8}{14} * 0.807 = 0.985$">

<br><!-- MATH
 $GainRatio(Windy) = \frac{0.048}{0.985} = 0.049$
 -->
<img width="292" height="39" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img15.png" alt="$GainRatio(Windy) = \frac{0.048}{0.985} = 0.049$">
</li>
</ul>
Otrzymali¶my wiêc bardziej zrównowa¿one wyniki. Mo¿emy zatem zamiast Gain u¿ywaæ GainRatio.
  
<h1><a name="SECTION00050000000000000000">
Algorytm ID3</a>
</h1>

<p>
Rekurencyjny algorytm budowy drzewa, który jako danych wej¶ciowych potrzebuje:

</p><ul>
<li>zbiór przyk³adowych rekordów = Training Set T
</li>
<li>kategoria = categorical attribute C
</li>
<li>zbiór atrybutów = non-categorical attributes C1, C2, .., Cn
</li>
</ul>

<p>
</p><pre>   function ID3 (R: a set of non-categorical attributes,
		 C: the categorical attribute,
		 S: a training set) returns a decision tree;
   begin
	If S is empty, return a single node with value Failure;
	If S consists of records all with the same value for 
	   the categorical attribute, 
	   return a single node with that value;
	If R is empty, then return a single node with as value
	   the most frequent of the values of the categorical attribute
	   that are found in records of S; [note that then there
	   will be errors, that is, records that will be improperly
	   classified];
	Let D be the attribute with largest Gain(D,S) 
	   among attributes in R;
	Let {dj| j=1,2, .., m} be the values of attribute D;
	Let {Sj| j=1,2, .., m} be the subsets of S consisting 
	   respectively of records with value dj for attribute D;
	Return a tree with root labeled D and arcs labeled 
	   d1, d2, .., dm going respectively to the trees 

	     ID3(R-{D}, C, S1), ID3(R-{D}, C, S2), .., ID3(R-{D}, C, Sm);
   end ID3;
</pre>
  
<h1><a name="SECTION00060000000000000000">
Cechy algorytmu ID3</a>
</h1>

<p>

</p><h4><a name="SECTION00060010000000000000">
Zalety</a>
</h4>

<ul>
<li>prostota
</li>
<li>je¿eli w <tt>Training Set</tt> nie ma tzw. ha³asu (ang. noise), czyli ¿e 
  nie ma rekordów które dla tych samych warto¶ci atrybutów maj± 
  przypisan± ró¿n± kategoriê, to ID3 daje poprawny wynik dla wszystkich
  rekordów z <tt>Training Set</tt>.
</li>
</ul>

<p>

</p><h4><a name="SECTION00060020000000000000">
Wady</a>
</h4>

<ul>
<li>nie radzi sobie z ci±g³ymi dziedzinami atrybutów (zak³ada, ¿e warto¶æi
  atrybutów s± dyskretne)
</li>
<li>zak³ada sztywno, ¿e wszystkie rekordy w <tt>Training Set</tt> s± wype³nione
  tzn. nie dzia³a, je¿eli choæ jeden rekord zawiera niepe³ne dane.
</li>
<li>du¿y rozmiar drzewa
</li>
<li>brak odporno¶ci na zjawisko zwane <i>overfitting</i>. Polega to na tym, 
  ¿e algorytm nie radzi sobie z danymi zaburzaj±cymi ogóln± ich informacjê. 
  Mo¿e to w rezultacie prowadziæ do wysokiego wspó³czynnika b³êdów na 
  danych testowych.
</li>
</ul>
  
<h1><a name="SECTION00070000000000000000">
C4.5, czyli dlaczego ID3 nam nie wystarczy?</a>
</h1>
Algorytm C4.5 (Quinlan) jest rozszerzeniem algorytmu ID3 wychodz±cym naprzeciw 
problemom napotkanym przez ID3. 

<p>

</p><h2><a name="SECTION00071000000000000000">
Lekarstwo na techniczne ograniczenia ID3</a>
</h2>

<ul>
<li><b>Buduj±c drzewo decyzyjne</b> mo¿emy rozwi±zaæ przypadek, gdy rekordy 
maj± nieznan± warto¶æ dla pewnych atrybutów. 
<br>
Gain jest wtedy wyliczane na podstawie tylko tych rekordów, gdzie dana
warto¶æ jest zdefiniowana.
</li>
<li><b>U¿ywaj±c drzewa decyzyjnego</b> umiemy sklasyfikowaæ rekordy z 
nieznan± warto¶ci± atrybutu poprzez wyliczenie prawdopodobieñstwa mo¿liwych
wyników. 
<br>
Przyk³adowo dla golfa we¼my rekord, gdzie znany jest Outlook, a nieznana 
Humidity. Widzimy, ¿e rozk³ad prawdopodobieñstwa jest (0.4, 0.6) dla 
(play, don't play). 
</li>
<li>Atrybuty mog± mieæ ci±g³e warto¶ci. 
<br>
Aby otrzymaæ podzia³ dyskretny postêpujemy nastêpuj±co:

<ul>
<li>Rozpatrujemy warto¶ci atrybutu w Training Set, ustawmy je rosn±co
od <img width="26" height="34" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img16.png" alt="$A_1$"> do <img width="31" height="34" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img17.png" alt="$A_m$">. 
</li>
<li>Dla ka¿dej warto¶ci dzielimy zbiór rekordów na te, które maj± warto¶æ
atrybutu <img width="44" height="34" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img18.png" alt="$\leq A_i$"> oraz na te, które maj± warto¶æ <img width="44" height="34" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img19.png" alt="$&gt; A_i$">.
</li>
<li>Wyliczamy gain, dla ka¿dego takiego podzia³u.
</li>
<li>Wybieramy ten podzia³, dla którego gain jest najwiêksze.
</li>
</ul>
Przyk³adowo dla golfa, je¶li rozwa¿amy Humidity, po wykonaniu wszystkich 
obliczeñ wychodzi nam, i¿ najlepszym punktem podzia³u bêdzie warto¶æ 75.
Jako warto¶ci dla atrybutu Humidity bêdziemy wiêc rozwa¿aæ przy budowie drzewa
zbiór <img width="90" height="33" align="MIDDLE" border="0" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/img20.png" alt="$\leq75, &gt; 75$">.
</li>
</ul>

<p>

</p><h2><a name="SECTION00072000000000000000">
Przycinanie, czyli g³ówny pomys³ C4.5</a>
</h2>
W algorytmie ID3 g³ownym k³opotem by³ niepotrzebny rozrost drzewa i brak 
mechanizmów przeciwdzia³aj±cych zjawisku <i>overfitting-u</i>, co prowadzi³o 
do do¶æ wysokiego poziomu b³êdów dla rzeczywistych danych.
Aby tego unikn±æ stosuje siê tzw.  <i>przycinanie (ang. pruning) </i>, w celu 
zwiêkszenia generalizacji oceny. Konkretnie dzia³a ono w nastêpuj±cy sposób:

<p>

</p><ul>
<li>zaczyna od li¶ci i dzia³a BottomUp
</li>
<li>maj±c dany wêze³ nie bêd±cy li¶ciem i jego poddrzewo oblicza w heurystyczny 
  sposób warto¶æ przewidywanego b³êdu dla aktualnego poddrzewa.
</li>
<li>oblicza warto¶æ przewidywanego b³êdu dla sytuacji, gdyby rozpatrywane 
  poddrzewo zast±piæ pojedynczym li¶ciem z kategori± najpopularniejsz± w¶ród
  li¶ci.
</li>
<li>porównuje te dwie warto¶ci i ewentualnie dokonuje zamiany poddrzewa na 
  pojedynczy li¶æ propaguj±c tê informacjê do swych przodków.
</li>
</ul>

<p>
Dziêki temu zabiegowi otrzymujemy wiêksz± generalizacjê oceny nowych 
przypadków, a o to przecie¿ nam chodzi³o.
  
</p><p>
  
</p><h1><a name="SECTION00080000000000000000">
Implementacja algorytmu C4.5</a>
</h1>
W sieci ogólnodostêpna jest implementacja algorytmu C4.5. Pakiet zawiera 
zarówno program do tworzenia drzewa decyzyjnego, jak i do tworzenia 
regu³ wnioskowania na podstawie wygenerowanego drzewa oraz prost± aplikacjê 
do zastosowania otrzymanych regu³ do klasyfikacji nowych przypadków.

<p>
Poni¿ej zaprezentujê efekty testów dla znajomych przyk³adów.

</p><p>

</p><h2><a name="SECTION00081000000000000000">
Nazewnictwo</a>
</h2>
Programy <tt>c4.5</tt> oraz <tt>c4.5rules</tt> jako domy¶lne parametry przyjmuj± 
pliki <tt>problem.names</tt> z zakodowanymi dziedzinami oraz <tt>problem.data</tt> z 
zakodowanym <tt>Training Set</tt>. Opcjonalnym parametrem jest zbiór danych 
testowych <tt>problem.test</tt>.

<p>

</p><h2><a name="SECTION00082000000000000000">
Wyniki</a>
</h2>

<h3><a name="SECTION00082100000000000000">
Problem gry w golfa</a>
</h3>
Oto dane i wyniki dla problemu z gr± w golfa:

<p>

</p><h4><a name="SECTION00082110000000000000">
Atrybuty i ich dziedziny</a>
</h4>

<p>
</p><pre>Play, Don't Play.

outlook: sunny, overcast, rain.
temperature: continuous.
humidity: continuous.
windy: true, false.
</pre>

<p>

</p><h4><a name="SECTION00082120000000000000">
Training Set</a>
</h4>

<p>
</p><pre>sunny, 85, 85, false, Don't Play
sunny, 80, 90, true, Don't Play
overcast, 83, 78, false, Play
rain, 70, 96, false, Play
rain, 68, 80, false, Play
rain, 65, 70, true, Don't Play
overcast, 64, 65, true, Play
sunny, 72, 95, false, Don't Play
sunny, 69, 70, false, Play
rain, 75, 80, false, Play
sunny, 75, 70, true, Play
overcast, 72, 90, true, Play
overcast, 81, 75, false, Play
rain, 71, 80, true, Don't Play
</pre>

<p>

</p><h4><a name="SECTION00082130000000000000">
Wyprodukowane drzewo</a>
</h4>

<p>
</p><pre>C4.5 [release 8] decision tree generator	Wed Mar 17 20:18:34 2004
----------------------------------------

    Options:
	File stem &lt;golf&gt;

Read 14 cases (4 attributes) from golf.data

Decision Tree:

outlook = overcast: Play (4.0)
outlook = sunny:
|   humidity &lt;= 75 : Play (2.0)
|   humidity &gt; 75 : Don't Play (3.0)
outlook = rain:
|   windy = true: Don't Play (2.0)
|   windy = false: Play (3.0)


Tree saved


Evaluation on training data (14 items):

	 Before Pruning           After Pruning
	----------------   ---------------------------
	Size      Errors   Size      Errors   Estimate

	   8    0( 0.0%)      8    0( 0.0%)    (38.5%)   &lt;&lt;
</pre>

<p>

</p><h4><a name="SECTION00082140000000000000">
Otrzymane regu³y</a>
</h4>

<p>
</p><pre>C4.5 [release 8] rule generator	Wed Mar 17 20:18:44 2004
-------------------------------

    Options:
	File stem &lt;golf&gt;

Read 14 cases (4 attributes) from golf

------------------
Processing tree 0

Final rules from tree 0:

Rule 2:
    	outlook = overcast
	-&gt;  class Play  [70.7%]&lt;br&gt;

Rule 4:
    	outlook = rain
    	windy = false
	-&gt;  class Play  [63.0%]

Rule 1:
    	outlook = sunny
    	humidity &gt; 75
	-&gt;  class Don't Play  [63.0%]

Rule 3:
    	outlook = rain
    	windy = true
	-&gt;  class Don't Play  [50.0%]

Default class: Play


Evaluation on training data (14 items):

Rule  Size  Error  Used  Wrong	          Advantage
----  ----  -----  ----  -----	          ---------
   2     1  29.3%     4      0 (0.0%)	     0 (0|0) 	Play
   4     2  37.0%     3      0 (0.0%)	     0 (0|0) 	Play
   1     2  37.0%     3      0 (0.0%)	     3 (3|0) 	Don't Play
   3     2  50.0%     2      0 (0.0%)	     2 (2|0) 	Don't Play

Tested 14, errors 0 (0.0%)   &lt;&lt;


	  (a)  (b)	&lt;-classified as
	 ---- ----
	    9     	(a): class Play
	         5	(b): class Don't Play
</pre>

<p>

</p><h4><a name="SECTION00082150000000000000">
Zabawa z interpreterem drzewa</a>
</h4>
<pre>C4.5 [release 8] decision tree interpreter      Thu Mar 18 20:59:20 2004
------------------------------------------

outlook: sunny
humidity: 23

Decision:
        Play  CF = 1.00  [ 0.50 - 1.00 ]
</pre>

<p>

</p><h3><a name="SECTION00082200000000000000">
Problem rozpoznawania jêzyka mówionego</a>
</h3>
Komputer ma rozpoznaæ, jak± pisowniê ma cz³owiek na my¶li wymawiaj±c 
identycznie brzmi±ce s³owa. W naszym przypadku rozwa¿amy s³owa 
<i>bare</i> i <i>bear</i>. Ma do dyspozycji informacjê o czê¶ciach mowy piêciu 
wyrazów poprzedzaj±cych s³owo, którego pisownie klasyfikujemy.

<p>

</p><h4><a name="SECTION00082210000000000000">
Atrybuty i ich dziedziny</a>
</h4>

<p>
</p><pre>bare, bear.

pos1: none, verb, noun, adjective, adverb, article, connective, number, possessive, pronoun, interjection, modal, preposition.
pos2: none, verb, noun, adjective, adverb, article, connective, number, possessive, pronoun, interjection, modal, preposition.
pos3: none, verb, noun, adjective, adverb, article, connective, number, possessive, pronoun, interjection, modal, preposition.
pos4: none, verb, noun, adjective, adverb, article, connective, number, possessive, pronoun, interjection, modal, preposition.
pos5: none, verb, noun, adjective, adverb, article, connective, number, possessive, pronoun, interjection, modal, preposition.
</pre>

<p>

</p><h4><a name="SECTION00082220000000000000">
Training Set</a>
</h4>

<p>
</p><pre>none, article, noun, verb, adverb, bare
none, none, none, none, article, bare
none, none, possessive, noun, verb, bare
none, article, noun, noun, verb, bare
verb, adverb, adverb, preposition, article, bare
none, none, none, verb, adverb, bare
none, none, none ,none, none, bare
none, none, none, none, article, bear
pronoun, verb, verb, preposition, article, bear
none, none, pronoun, verb, article, bear
none, none, adverb, noun, modal, bear
none, none, adverb, noun, modal, bear
none, pronoun, modal, pronoun, preposition, bear
pronoun, verb, verb, pronoun, modal, bear
none, none, none, verb, preposition, bear
none, none, none, pronoun, modal, bear
</pre>

<p>

</p><h4><a name="SECTION00082230000000000000">
Wyprodukowane drzewo</a>
</h4>

<p>
</p><pre>C4.5 [release 8] decision tree generator	Wed Mar 17 20:25:26 2004
----------------------------------------

    Options:
	File stem &lt;bare&gt;

Read 16 cases (5 attributes) from bare.data

Decision Tree:

pos5 = none: bare (1.0)
pos5 = verb: bare (2.0)
pos5 = noun: bear (0.0)
pos5 = adjective: bear (0.0)
pos5 = adverb: bare (2.0)
pos5 = article: bear (5.0/2.0)
pos5 = connective: bear (0.0)
pos5 = number: bear (0.0)
pos5 = possessive: bear (0.0)
pos5 = pronoun: bear (0.0)
pos5 = interjection: bear (0.0)
pos5 = modal: bear (4.0)
pos5 = preposition: bear (2.0)


Tree saved


Evaluation on training data (16 items):

	 Before Pruning           After Pruning
	----------------   ---------------------------
	Size      Errors   Size      Errors   Estimate

	  14    2(12.5%)     14    2(12.5%)    (51.0%)   &lt;&lt;
</pre>

<p>
Jak widaæ program po czasowniku zawsze bêdzie mówi³, ¿e chodzi³o o misia, 
za¶ po czasowniku modalnym stwierdzi, ¿e mieli¶my na my¶li tolerowanie czego¶.

</p><h1><a name="SECTION00090000000000000000">
About this document ...</a>
</h1>
 <strong>Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a</strong><p>
This document was generated using the
<a href="http://www.latex2html.org/"><strong>LaTeX</strong>2<tt>HTML</tt></a> translator Version 2002 (1.62)
</p><p>
Copyright  1993, 1994, 1995, 1996,
<a href="http://cbl.leeds.ac.uk/nikos/personal.html">Nikos Drakos</a>, 
Computer Based Learning Unit, University of Leeds.
<br>
Copyright  1997, 1998, 1999,
<a href="http://www.maths.mq.edu.au/~ross/">Ross Moore</a>, 
Mathematics Department, Macquarie University, Sydney.
</p><p>
The command line arguments were: <br>
 <strong>latex2html</strong> <tt>-split 0 -local_icons -html_version 3.2,latin2,unicode c4_5Main.tex</tt>
</p><p>
The translation was initiated by Przemyslaw Strzelczak on 2004-03-19</p><hr>
<!--Navigation Panel-->
<img width="81" height="24" align="BOTTOM" border="0" alt="next_inactive" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/nx_grp_g.png"> 
<img width="26" height="24" align="BOTTOM" border="0" alt="up" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/up_g.png"> 
<img width="63" height="24" align="BOTTOM" border="0" alt="previous" src="./Drzewo decyzyjne C4.5, czyli jak nauczyæ komputer odró¿niaæ dobro od z³a_files/prev_g.png">   
<br>
<!--End of Navigation Panel-->
<address>
Przemyslaw Strzelczak
2004-03-19
</address>


</body></html>